{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A9-mm05OgNeeEwvuX69PtIiGjGybD-wy",
      "authorship_tag": "ABX9TyOenc8KkIIXnI7hWsFrtelP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H-Levi/NLP_Project1/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0p2VwFo5eU8D"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import wikipedia\n",
        "import wikipediaapi\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Assign a user_agent\n",
        "user_agent = 'hailegabriel/1.0 (bentitan98@gmail.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia(user_agent, 'en', wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "\n",
        "my_file = {}\n",
        "# Search for titles in wikipedia\n",
        "for items in wikipedia.search('Medical', results=360):\n",
        "    # Extract the summary of the content of the selected titles\n",
        "    my_file[items] = wiki_wiki.page(items).summary\n",
        "# create a JSON file and write the content summary i\n",
        "with open('medical.json', 'w') as medical:\n",
        "    json.dump(my_file, medical, indent=4)\n",
        "\n",
        "user_agent = 'hailegabriel/1.0 (bentitan98@gmail.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia(user_agent, 'en', wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "my_file = {}\n",
        "non_medical_titles = ['Sport', 'Technology', 'Art', 'Entertainment', 'Finance', 'Travel', 'Fashion', 'Business', 'Education']\n",
        "\n",
        "for content in non_medical_titles:\n",
        "    for items in wikipedia.search(content, results=40):\n",
        "        key = f\"{content}_{items}\"  # Use a unique key based on content type and title\n",
        "        my_file[key] = wiki_wiki.page(items).summary\n",
        "\n",
        "with open('non_medical.json', 'w') as non_medical:\n",
        "    json.dump(my_file, non_medical, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api\n",
        "gi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CeBjbSUe9Rb",
        "outputId": "a88a9393-e3c5-4f0a-9cc3-fefd7acadde8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2023.7.22)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load medical data from JSON\n",
        "with open('medical.json', 'r') as file1:\n",
        "    medical_data_dict = json.load(file1)\n",
        "    medical_data = list(medical_data_dict.values())\n",
        "\n",
        "# Load non-medical data from JSON\n",
        "with open('non_medical.json', 'r') as file2:\n",
        "    non_medical_data_dict = json.load(file2)\n",
        "    non_medical_data = list(non_medical_data_dict.values())\n",
        "\n",
        "\n",
        "# Preprocess data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]  # Remove non-alphabetic characters\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
        "    words = [stemmer.stem(word) for word in words]  # Stemming\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
        "    return words\n",
        "\n",
        "# Extract features from text\n",
        "def extract_features(text):\n",
        "    features = {}\n",
        "    for word in preprocess_text(text):\n",
        "        features[word] = True\n",
        "    return features\n",
        "\n",
        "# Create labeled dataset to label as well as combine both data sets into one\n",
        "labeled_data = [(text, 'medical') for text in medical_data] + [(text, 'nonmedical') for text in non_medical_data]\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_set, test_set = train_test_split(labeled_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_classifier = NaiveBayesClassifier.train([(extract_features(text), label) for (text, label) in train_set])\n",
        "\n",
        "# Evaluate Naive Bayes classifier\n",
        "nb_predictions = [nb_classifier.classify(extract_features(text)) for (text, label) in test_set]\n",
        "nb_accuracy = accuracy_score([label for (text, label) in test_set], nb_predictions)\n",
        "nb_report = classification_report([label for (text, label) in test_set], nb_predictions)\n",
        "\n",
        "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
        "print(\"Naive Bayes Report:\\n\", nb_report)\n",
        "\n",
        "# Additional testing sets for manual test\n",
        "additional_sentences = [\n",
        "    \"I love to play football.\",\n",
        "    \"The human body has 206 bones.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"Artificial intelligence is reshaping industries.\",\n",
        "    \"The stock market experienced a sharp decline yesterday.\",\n",
        "]\n",
        "\n",
        "# Test the models with additional sentences\n",
        "for sentence in additional_sentences:\n",
        "    # Extract features from the sentence\n",
        "    features = extract_features(sentence)\n",
        "\n",
        "    # Predict with Naive Bayes\n",
        "    nb_prediction = nb_classifier.classify(features)\n",
        "    print(f\"Naive Bayes Prediction for '{sentence}': {nb_prediction}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cNpNSDQeZg4",
        "outputId": "de32e381-7d4f-4361-f9f7-a031dde516dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.9305555555555556\n",
            "Naive Bayes Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.97      0.90      0.94        82\n",
            "  nonmedical       0.88      0.97      0.92        62\n",
            "\n",
            "    accuracy                           0.93       144\n",
            "   macro avg       0.93      0.94      0.93       144\n",
            "weighted avg       0.93      0.93      0.93       144\n",
            "\n",
            "Naive Bayes Prediction for 'I love to play football.': nonmedical\n",
            "Naive Bayes Prediction for 'The human body has 206 bones.': medical\n",
            "Naive Bayes Prediction for 'The capital of France is Paris.': nonmedical\n",
            "Naive Bayes Prediction for 'Artificial intelligence is reshaping industries.': nonmedical\n",
            "Naive Bayes Prediction for 'The stock market experienced a sharp decline yesterday.': nonmedical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_f9BBI_f4hd",
        "outputId": "d3ce2a9b-ef3f-4376-89f4-f8ef4f59163f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}